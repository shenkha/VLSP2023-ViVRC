{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6453703,"sourceType":"datasetVersion","datasetId":3726200},{"sourceId":6529633,"sourceType":"datasetVersion","datasetId":3774852}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !git clone https://github.com/huggingface/transformers.git","metadata":{"execution":{"iopub.status.busy":"2023-09-25T04:55:05.988028Z","iopub.execute_input":"2023-09-25T04:55:05.988389Z","iopub.status.idle":"2023-09-25T04:55:05.99721Z","shell.execute_reply.started":"2023-09-25T04:55:05.988359Z","shell.execute_reply":"2023-09-25T04:55:05.996205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n!pip install datasets evaluate transformers[sentencepiece]\n!pip install rouge_score\n!pip install underthesea\n!pip install pyvi","metadata":{"execution":{"iopub.status.busy":"2023-09-25T04:55:05.999724Z","iopub.execute_input":"2023-09-25T04:55:06.000454Z","iopub.status.idle":"2023-09-25T04:56:04.382884Z","shell.execute_reply.started":"2023-09-25T04:55:06.000422Z","shell.execute_reply":"2023-09-25T04:56:04.381645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n!pip install gdown\nimport gdown\n\n!gdown 1jWDnP1xF01_pmeJYsRHxSVBkLZ0I9LAt #utils T5 for VQA\n!gdown 1jHGEuwjMI_P81Ha-L3o5IK6hAu6YQAjG #easyocr\n!gdown 1WKJC33RpxTot6YRryVwBuQCRhkehsM2g #scoring\n!unzip -o scoring_program.zip","metadata":{"execution":{"iopub.status.busy":"2023-09-25T04:56:04.386379Z","iopub.execute_input":"2023-09-25T04:56:04.387037Z","iopub.status.idle":"2023-09-25T04:56:31.213869Z","shell.execute_reply.started":"2023-09-25T04:56:04.387001Z","shell.execute_reply":"2023-09-25T04:56:31.212562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nf = open('/kaggle/input/vlsp2023-dsvivrc/training-data/training-data/vlsp2023_train_data.json')\ntrain_json = json.load(f)\nf = open('/kaggle/input/vlsp2023-dsvivrc/dev-set/dev-set/vlsp2023_dev_data.json')\ndev_json = json.load(f)\nf = open('/kaggle/working/easyocr.json')\neasyocr_json = json.load(f)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T04:56:31.215871Z","iopub.execute_input":"2023-09-25T04:56:31.216662Z","iopub.status.idle":"2023-09-25T04:56:31.474342Z","shell.execute_reply.started":"2023-09-25T04:56:31.216626Z","shell.execute_reply":"2023-09-25T04:56:31.473376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%%capture\n%cd /kaggle/working\nimport os\nimport shutil\n\nsource_all = ['/kaggle/input/vlsp2023-dsvivrc/training-data/training-data/training-images/training-images',\n              '/kaggle/input/vlsp2023-dsvivrc/dev-set/dev-set/dev-images/dev-images']\ndestination = './images'\nos.mkdir(destination)\n\nfor source in source_all:\n    allfiles = os.listdir(source)\n    for f in allfiles:\n        src_path = os.path.join(source, f)\n        dst_path = os.path.join(destination, f)\n        shutil.copy(src_path, dst_path)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T04:56:31.477133Z","iopub.execute_input":"2023-09-25T04:56:31.477507Z","iopub.status.idle":"2023-09-25T04:57:53.782575Z","shell.execute_reply.started":"2023-09-25T04:56:31.477473Z","shell.execute_reply":"2023-09-25T04:57:53.781511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ntrain_set = pd.DataFrame(train_json['annotations']).T\ndev_set = pd.DataFrame(dev_json['annotations']).T\n\ntrain_set = train_set[['question','answer','image_id']]\ntrain_set['image_id'] = train_set['image_id'].astype(str)\ndev_set = dev_set[['question','answer','image_id']]\ndev_set['image_id'] = dev_set['image_id'].astype(str)\n\ntrain_set['question'] = train_set['question'].str.replace(' ?','') + \" \"+ train_set['image_id'].apply(lambda x: easyocr_json[x])\ndev_set['question'] = dev_set['question'].str.replace(' ?','') + \" \"+ dev_set['image_id'].apply(lambda x: easyocr_json[x])\n\nimg_dict_train = train_json['images']\nimg_dict_dev = dev_json['images']\nimg_dict = dict(img_dict_train)\nimg_dict.update(img_dict_dev)\n\nprint(len(img_dict_train))\nprint(len(img_dict_dev))\nprint(len(img_dict))","metadata":{"execution":{"iopub.status.busy":"2023-09-25T04:57:53.785558Z","iopub.execute_input":"2023-09-25T04:57:53.786267Z","iopub.status.idle":"2023-09-25T04:57:55.490284Z","shell.execute_reply.started":"2023-09-25T04:57:53.78623Z","shell.execute_reply":"2023-09-25T04:57:55.489307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from PIL import Image\n# import tqdm\n# from transformers import (\n#     AutoTokenizer, AutoFeatureExtractor,\n#     AutoModel,            \n#     TrainingArguments,\n#     logging\n# )\n# import torch\n\n# #google/vit-base-patch16-224-in21k\n# #google/vit-large-patch32-384\n# model_name = 'google/vit-base-patch16-224-in21k'\n# #model_name ='microsoft/beit-large-patch16-224'\n# image_encoder = AutoModel.from_pretrained(model_name)\n# preprocessor = AutoFeatureExtractor.from_pretrained(model_name)\n\n# img_w = {}\n# for k,v in tqdm.tqdm_notebook(img_dict.items()):\n#     imgl = '/kaggle/working/images/' + v\n#     processed_images = preprocessor(images=[Image.open(imgl).convert('RGB')],\n#                                     return_tensors=\"pt\",)['pixel_values']\n#     with torch.no_grad():\n#         encoded_image = image_encoder(pixel_values=processed_images, return_dict=True).last_hidden_state[:,1:,:].squeeze()# we discard the CLS token\n#         #encoded_image = encoded_image.last_hidden_state[:,1:,:].squeeze() \n#     img_w[k] = encoded_image#.to('cpu')\n#     del encoded_image\n#     torch.cuda.empty_cache()\n# torch.save(img_w, '/kaggle/working/vit-b.pt') # export for later used\n\n\n# # imgl = '/kaggle/working/images/' + img_dict[int(train_set.image_id[45])]\n# # processed_images = preprocessor(images=[Image.open(imgl).convert('RGB')],\n# #                                 return_tensors=\"pt\",)['pixel_values']\n# # encoded_image = image_encoder(pixel_values=processed_images,\n# #                               return_dict=True)\n\n# # last_hidden_state = encoded_image.last_hidden_state[:,1:,:] # we discard the CLS token\n# # last_hidden_state.squeeze().size()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T04:57:55.491721Z","iopub.execute_input":"2023-09-25T04:57:55.492443Z","iopub.status.idle":"2023-09-25T04:57:55.498495Z","shell.execute_reply.started":"2023-09-25T04:57:55.492408Z","shell.execute_reply":"2023-09-25T04:57:55.49752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from IPython.display import FileLink\n\n# FileLink(r'vit-b.pt')","metadata":{"execution":{"iopub.status.busy":"2023-09-25T04:57:55.499692Z","iopub.execute_input":"2023-09-25T04:57:55.500264Z","iopub.status.idle":"2023-09-25T04:57:55.510508Z","shell.execute_reply.started":"2023-09-25T04:57:55.500232Z","shell.execute_reply":"2023-09-25T04:57:55.509587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"load image features extracted from ViT model","metadata":{}},{"cell_type":"code","source":"import torch\n\n#img_w = torch.load('/kaggle/input/vlsp2023-vivrc-beit/beit.pt') # already-saved features\nimg_w = torch.load('/kaggle/input/vlsp2023-vivrc-vitb/vit-b.pt') # already-saved features\nlen(img_w)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T04:57:55.511991Z","iopub.execute_input":"2023-09-25T04:57:55.512317Z","iopub.status.idle":"2023-09-25T04:58:48.652423Z","shell.execute_reply.started":"2023-09-25T04:57:55.512287Z","shell.execute_reply":"2023-09-25T04:58:48.651385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom utils import T5ForConditionalGeneration\ntokenizer = AutoTokenizer.from_pretrained(\"VietAI/vit5-base\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"VietAI/vit5-base\").to('cuda')\nmodel.add_imgw(img_w)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T04:58:48.653992Z","iopub.execute_input":"2023-09-25T04:58:48.654593Z","iopub.status.idle":"2023-09-25T04:59:52.028526Z","shell.execute_reply.started":"2023-09-25T04:58:48.654559Z","shell.execute_reply":"2023-09-25T04:59:52.027513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, TrainingArguments, Seq2SeqTrainingArguments\nfrom tqdm.notebook import tqdm\nfrom torch.utils.data import DataLoader\n\ndef preprocess_function(examples):\n    model_inputs = tokenizer(\n        examples[\"inputs\"], max_length=128, truncation=True, padding=True\n    )\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            examples[\"labels\"], max_length=50, truncation=True, padding=True\n        )\n    model_inputs['labels'] = labels['input_ids']\n    model_inputs['input_ids'] = model_inputs['input_ids']\n    model_inputs[\"image_id\"] = examples[\"image_id\"]\n    \n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2023-09-25T04:59:52.033075Z","iopub.execute_input":"2023-09-25T04:59:52.033386Z","iopub.status.idle":"2023-09-25T04:59:52.998651Z","shell.execute_reply.started":"2023-09-25T04:59:52.03336Z","shell.execute_reply":"2023-09-25T04:59:52.997661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dict_obj = {}\ndict_obj['inputs'] = train_set['question'] #[v['question'].replace(' ?','') for _,v in train_json['annotations'].items()]\ndict_obj['labels'] =  train_set['answer'] #[v['answer'] for _,v in train_json['annotations'].items()]\ndict_obj['image_id'] = train_set['image_id']\ntrain_dataset = Dataset.from_dict(dict_obj)\ntokenized_train_datasets = train_dataset.map(preprocess_function, batched=True, remove_columns=['inputs'], num_proc=8)\n\ndict_obj = {}\ndict_obj['inputs'] = dev_set['question']#[v['question'].replace(' ?','') for _,v in dev_json['annotations'].items()]\ndict_obj['labels'] = dev_set['answer']#[v['answer'] for _,v in dev_json['annotations'].items()]\ndict_obj['image_id'] = dev_set['image_id']\ndev_dataset = Dataset.from_dict(dict_obj)\ntokenized_dev_datasets = dev_dataset.map(preprocess_function, batched=True, remove_columns=['inputs'], num_proc=8)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T04:59:53Z","iopub.execute_input":"2023-09-25T04:59:53.000869Z","iopub.status.idle":"2023-09-25T05:00:04.650715Z","shell.execute_reply.started":"2023-09-25T04:59:53.000834Z","shell.execute_reply":"2023-09-25T05:00:04.649566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom transformers.optimization import Adafactor, AdafactorSchedule\nfrom utils import DataCollatorForSeq2Seq\n\nos.environ[\"WANDB_DISABLED\"] = \"True\"\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"pt\")\n\n#Adam\ntraining_args = Seq2SeqTrainingArguments(output_dir=\"./checkpoint\",\n                                      do_train=True,\n                                      do_eval=True,\n                                      num_train_epochs=30,\n                                      learning_rate=2.5e-5,\n                                      warmup_ratio=0.05,\n                                      weight_decay=0.01,\n                                      per_device_train_batch_size=32,\n                                      per_device_eval_batch_size=32,\n                                      logging_dir='./log',\n                                      group_by_length=True,\n                                      save_strategy=\"steps\",\n                                      save_total_limit=5,\n                                      eval_steps=100,\n                                      logging_steps = 100,\n                                      evaluation_strategy=\"steps\",\n                                      save_steps=100,\n                                      load_best_model_at_end= True,\n                                      fp16=True,\n                                      seed=42,\n                                      )\n\n# AdaFactor for ViT5-large models as it based on T5v1.1.\n# See https://medium.com/the-artificial-impostor/paper-adafactor-adaptive-learning-rates-with-sublinear-memory-cost-a543abffa37\n# \n# from transformers.optimization import Adafactor, AdafactorSchedule\n# optimizer = Adafactor(\n#     model.parameters(),\n#     lr=1e-3,\n#     eps=(1e-30, 1e-3),\n#     clip_threshold=1.0,\n#     decay_rate=-0.8,\n#     beta1=None,\n#     weight_decay=0.0,\n#     relative_step=False,\n#     scale_parameter=False,\n#     warmup_init=False\n# )\n# lr_scheduler = AdafactorSchedule(optimizer)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T05:00:04.654718Z","iopub.execute_input":"2023-09-25T05:00:04.655013Z","iopub.status.idle":"2023-09-25T05:00:04.668502Z","shell.execute_reply.started":"2023-09-25T05:00:04.654985Z","shell.execute_reply":"2023-09-25T05:00:04.667513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_datasets,\n    eval_dataset=tokenized_dev_datasets,\n    data_collator=data_collator,\n    #optimizers=(optimizer, lr_scheduler),\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T05:00:04.670247Z","iopub.execute_input":"2023-09-25T05:00:04.670683Z","iopub.status.idle":"2023-09-25T06:55:16.596995Z","shell.execute_reply.started":"2023-09-25T05:00:04.670646Z","shell.execute_reply":"2023-09-25T06:55:16.594542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir('/kaggle/working/checkpoint')","metadata":{"execution":{"iopub.status.busy":"2023-09-25T06:55:19.82664Z","iopub.execute_input":"2023-09-25T06:55:19.827035Z","iopub.status.idle":"2023-09-25T06:55:19.834503Z","shell.execute_reply.started":"2023-09-25T06:55:19.827001Z","shell.execute_reply":"2023-09-25T06:55:19.833573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\nmodel = T5ForConditionalGeneration.from_pretrained(\"/kaggle/working/checkpoint/checkpoint-5300\")#5300:2.9312\nmodel.to('cuda')\nmodel.add_imgw(img_w)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T06:55:27.051047Z","iopub.execute_input":"2023-09-25T06:55:27.051413Z","iopub.status.idle":"2023-09-25T06:55:36.353196Z","shell.execute_reply.started":"2023-09-25T06:55:27.051385Z","shell.execute_reply":"2023-09-25T06:55:36.35209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_metric\nmetric = load_metric(\"rouge\")","metadata":{"execution":{"iopub.status.busy":"2023-09-25T06:55:36.358523Z","iopub.execute_input":"2023-09-25T06:55:36.360852Z","iopub.status.idle":"2023-09-25T06:55:38.236483Z","shell.execute_reply.started":"2023-09-25T06:55:36.360807Z","shell.execute_reply":"2023-09-25T06:55:38.23494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from underthesea import word_tokenize\nfrom underthesea import text_normalize\nfrom pyvi import ViUtils\n\ndef postprocessing(t):\n    words = t.split()\n    words_not = words[:int(len(words)/3)]\n    words_dup = words[int(len(words)/3):]\n    words_dup = sorted(set(words_dup), key=words_dup.index)\n    \n    words = \" \".join(words_not+words_dup)\n    words = text_normalize(words)\n    #words = ViUtils.add_accents(words)\n    return words.lower()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T06:55:38.242081Z","iopub.execute_input":"2023-09-25T06:55:38.242505Z","iopub.status.idle":"2023-09-25T06:55:39.681597Z","shell.execute_reply.started":"2023-09-25T06:55:38.242452Z","shell.execute_reply":"2023-09-25T06:55:39.680377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch \nimport numpy as np\nmetrics = load_metric('rouge')\n\nmax_target_length = 30\ndataloader = torch.utils.data.DataLoader(tokenized_dev_datasets, collate_fn=data_collator, batch_size=32)\n\npredictions = []\nreferences = []\n\nfor i, batch in enumerate(tqdm(dataloader)):\n    outputs = model.generate(image_id = batch['image_id'],\n        input_ids=batch['input_ids'].to('cuda'),\n        max_length=max_target_length,\n        attention_mask=batch['attention_mask'].to('cuda'),\n        return_dict_in_generate=True, output_attentions=True)\n    with tokenizer.as_target_tokenizer():\n        outputs = [tokenizer.decode(out, clean_up_tokenization_spaces=False, skip_special_tokens=True) for out in outputs.sequences]\n        labels = np.where(batch['labels'] != -100,  batch['labels'], tokenizer.pad_token_id)\n        actuals = [tokenizer.decode(out, clean_up_tokenization_spaces=False, skip_special_tokens=True) for out in labels]\n    \n    \n    #post-processing\n    \n    #outputs = [postprocessing(t) for t in outputs]\n    \n    predictions.extend(outputs)\n    references.extend(actuals)\n    metrics.add_batch(predictions=outputs, references=actuals)\n\nmetrics.compute()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-25T06:55:39.687904Z","iopub.execute_input":"2023-09-25T06:55:39.690595Z","iopub.status.idle":"2023-09-25T06:56:57.819291Z","shell.execute_reply.started":"2023-09-25T06:55:39.690558Z","shell.execute_reply":"2023-09-25T06:56:57.818279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"[{k: v.mid.fmeasure} for k,v in metrics.compute(predictions=predictions, references=references).items()]","metadata":{"execution":{"iopub.status.busy":"2023-09-25T06:56:57.820758Z","iopub.execute_input":"2023-09-25T06:56:57.821922Z","iopub.status.idle":"2023-09-25T06:57:00.018197Z","shell.execute_reply.started":"2023-09-25T06:56:57.821883Z","shell.execute_reply":"2023-09-25T06:57:00.017173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from evaluation.bleu import Bleu\nfrom evaluation.cider import Cider\n\nimport numpy as np\nimport json\n\ndef compute_scores(gts, gens):\n    bleu = Bleu()\n    cider = Cider()\n    bleu_scores, _ = bleu.compute_score(gts, gens)\n    avg_bleu_score = np.array(bleu_scores).mean()\n    cider_score, _ = cider.compute_score(gts, gens)\n\n    return {\n        \"BLEU-1\": bleu_scores[0],\n        \"BLEU-2\": bleu_scores[1],\n        \"BLEU-3\": bleu_scores[2],\n        \"BLEU-4\": bleu_scores[3], \n        \"BLEU\": avg_bleu_score, \n        \"CIDEr\": cider_score\n    }\n\ndef evaluate(truth_file: str, submission_file, code_phase) -> dict:\n    gts = json.load(open(truth_file))\n    gts = {id: [answer] for id, answer in gts.items()}\n\n    gens = json.load(open(submission_file))\n    gens = {id: [answer] for id, answer in gens.items()}\n\n    if code_phase == \"public_test\":\n        return compute_scores(gts, gens)\n    else:\n        gens = {id: answer for id, answer in gens.items() if id in gts}\n        return compute_scores(gts, gens)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T06:57:00.019684Z","iopub.execute_input":"2023-09-25T06:57:00.020269Z","iopub.status.idle":"2023-09-25T06:57:00.046173Z","shell.execute_reply.started":"2023-09-25T06:57:00.020233Z","shell.execute_reply":"2023-09-25T06:57:00.045215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dev_groundtruth = {k:i['labels'] for k,i in zip(dev_json['annotations'].keys(),dev_dataset)}\ndev_predicted = {k:i for k,i in zip(dev_json['annotations'].keys(),predictions)}","metadata":{"execution":{"iopub.status.busy":"2023-09-25T06:57:00.047686Z","iopub.execute_input":"2023-09-25T06:57:00.048045Z","iopub.status.idle":"2023-09-25T06:57:00.294555Z","shell.execute_reply.started":"2023-09-25T06:57:00.048012Z","shell.execute_reply":"2023-09-25T06:57:00.293588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\nwith open('submission.json', 'w') as fp:\n    json.dump(dev_predicted, fp,ensure_ascii=True,indent=True)\n\nwith open('vls2023_openvivqa_dev_answers.json', 'w') as fp:\n    json.dump(dev_groundtruth , fp,ensure_ascii=True, indent=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T06:57:00.29611Z","iopub.execute_input":"2023-09-25T06:57:00.296509Z","iopub.status.idle":"2023-09-25T06:57:00.319028Z","shell.execute_reply.started":"2023-09-25T06:57:00.296474Z","shell.execute_reply":"2023-09-25T06:57:00.318139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nr = evaluate('/kaggle/working/vls2023_openvivqa_dev_answers.json',\n         '/kaggle/working/submission.json',\n         'public_test')\nr = {k:[v] for k,v in r.items()}\ndf_result = pd.DataFrame(data=r)\nhtml = df_result.style.set_table_styles([{'selector': 'th', 'props': [('font-size', '13pt')]}]).set_properties(**{'font-size': '13pt'}).format('{:.4f}')\nhtml","metadata":{"execution":{"iopub.status.busy":"2023-09-25T06:57:00.320569Z","iopub.execute_input":"2023-09-25T06:57:00.320925Z","iopub.status.idle":"2023-09-25T06:57:01.932422Z","shell.execute_reply.started":"2023-09-25T06:57:00.320893Z","shell.execute_reply":"2023-09-25T06:57:01.931387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r checkpoint_5300.zip /kaggle/working/checkpoint/checkpoint-5300","metadata":{"execution":{"iopub.status.busy":"2023-09-25T06:57:01.936219Z","iopub.execute_input":"2023-09-25T06:57:01.936763Z","iopub.status.idle":"2023-09-25T06:59:39.2961Z","shell.execute_reply.started":"2023-09-25T06:57:01.936736Z","shell.execute_reply":"2023-09-25T06:59:39.294666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'checkpoint_5300.zip')","metadata":{"execution":{"iopub.status.busy":"2023-09-25T06:59:39.301475Z","iopub.execute_input":"2023-09-25T06:59:39.301804Z","iopub.status.idle":"2023-09-25T06:59:39.313797Z","shell.execute_reply.started":"2023-09-25T06:59:39.301775Z","shell.execute_reply":"2023-09-25T06:59:39.312856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nfrom IPython.display import Image, display\nimport random\n\ndev_set_pred = dev_set.copy().reset_index().drop('index',axis=1)\ndev_set_pred['preds'] = dev_predicted.values()\n\ndef show_example():\n    a_sample = dev_set_pred.sample(1)\n    \n    q = a_sample['question'].values[0]\n    gt = a_sample['answer'].values[0]\n    img = a_sample['image_id'].values[0]\n    p = a_sample['preds'].values[0]\n    \n    img_p = '/kaggle/working/images/' + img_dict[str(img)]\n    display(Image(img_p, width=300))\n    print(a_sample.index[0])\n    \n    print('Image id: ',img)\n    print('Question:',q)\n    print('Groundtruth:',gt)\n    print(\"Predicted:\",p)\n    #print('Metrics: ',result[a_sample.index[0]])\n\nshow_example()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T06:59:39.31489Z","iopub.execute_input":"2023-09-25T06:59:39.315525Z","iopub.status.idle":"2023-09-25T06:59:39.396757Z","shell.execute_reply.started":"2023-09-25T06:59:39.31549Z","shell.execute_reply":"2023-09-25T06:59:39.395937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(img,q):\n    \n    q1 = tokenizer.encode(q,return_tensors='pt')\n    attention_mask = torch.ones_like(q1)\n    \n    outputs = model.generate(image_id = img,\n        input_ids=q1.to('cuda'),\n        max_length=50,\n        attention_mask=attention_mask.to('cuda'),\n        return_dict_in_generate=True, output_attentions=True)\n    with tokenizer.as_target_tokenizer():\n        outputs1 = [tokenizer.decode(out, clean_up_tokenization_spaces=False, skip_special_tokens=True) for out in outputs.sequences]\n    \n    out = model(input_ids=q1.cuda(),image_id = img,\n                attention_mask=attention_mask.to('cuda'),\n                decoder_input_ids=outputs.sequences, output_attentions=True, return_dict=True)\n\n    encoder_attentions = out.encoder_attentions\n    cross_attentions = out.cross_attentions\n    decoder_attentions = out.decoder_attentions\n    return q1,outputs1, cross_attentions","metadata":{"execution":{"iopub.status.busy":"2023-09-25T06:59:39.397909Z","iopub.execute_input":"2023-09-25T06:59:39.398279Z","iopub.status.idle":"2023-09-25T06:59:39.406415Z","shell.execute_reply.started":"2023-09-25T06:59:39.398241Z","shell.execute_reply":"2023-09-25T06:59:39.405521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nfrom IPython.display import display\n\nq = 'đây là đâu'\nidx = random.choice(list(img_dict.keys()))\n#idx ='8336'\n#idx = '4179'\n#idx = '111'\n#idx = '8701'\n#idx = '5775'\n#idx = '3717'\nprint(idx)\nimg = [idx]\n\ndisplay(Image.open('/kaggle/working/images/'+img_dict[img[0]]))\nq,a,attn = predict(img,q)\nq = [tokenizer.convert_ids_to_tokens(i) for i in q[0].tolist()]\nprint(a)\n\nimport cv2\nimport matplotlib.pyplot as plt\n\nfor i in range(100):\n    plt.figure()\n    try:\n        attn_img = attn[-1][0][-1,i,:196].detach().cpu()\n    except:\n        break\n    attn_img = torch.reshape(attn_img , (-1,14)).cpu()\n\n\n    tmp = cv2.imread('/kaggle/working/images/'+img_dict[img[0]])\n    tmp = cv2.resize(np.array(tmp), (224,224))\n    heatmap = cv2.resize(np.array(attn_img) , (224,224))\n    heatmapshow = None\n    heatmapshow = cv2.normalize(heatmap, heatmapshow, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n    heatmapshow = cv2.applyColorMap(heatmapshow, cv2.COLORMAP_JET)\n\n    final = cv2.addWeighted(src1=tmp,alpha=0.5,src2=heatmapshow,beta=0.5,gamma=0)\n\n    plt.imshow(final)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T07:56:38.733568Z","iopub.execute_input":"2023-09-25T07:56:38.733987Z","iopub.status.idle":"2023-09-25T07:56:41.751365Z","shell.execute_reply.started":"2023-09-25T07:56:38.733956Z","shell.execute_reply":"2023-09-25T07:56:41.750413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dev_predicted","metadata":{"execution":{"iopub.status.busy":"2023-09-25T06:59:42.999425Z","iopub.execute_input":"2023-09-25T06:59:43.000615Z","iopub.status.idle":"2023-09-25T06:59:43.043444Z","shell.execute_reply.started":"2023-09-25T06:59:43.000577Z","shell.execute_reply":"2023-09-25T06:59:43.042394Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nattn[-1][-1][-1,:-1,:196].size()#.size()\nfor i in range(12):\n    sns.heatmap(attn[-1][0][i,:,196:].detach().cpu(),cmap='BuPu')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T07:57:32.052293Z","iopub.execute_input":"2023-09-25T07:57:32.052702Z","iopub.status.idle":"2023-09-25T07:57:35.314276Z","shell.execute_reply.started":"2023-09-25T07:57:32.052669Z","shell.execute_reply":"2023-09-25T07:57:35.313276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"while True:\n    print(60)\n    time.sleep(60)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T08:12:22.867855Z","iopub.execute_input":"2023-09-25T08:12:22.868253Z"},"trusted":true},"execution_count":null,"outputs":[]}]}